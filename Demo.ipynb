{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "475b0e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "# Below code prevents pre-allocating whole available GPU memory.\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    for idx in range(len(physical_devices)):\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[idx], True)\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass\n",
    "\n",
    "from utils_model import MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d87f236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'N': 6, 'H': 12, 'D_MODEL': 768, 'DROPOUT': 0, 'SOURCE_LEN': 256, 'TARGET_LEN': 256, 'VOCAB_SIZE': 256}\n"
     ]
    }
   ],
   "source": [
    "with open('config_large.json', 'r') as openfile:\n",
    "    config = json.load(openfile)\n",
    "print(config)\n",
    "\n",
    "SOURCE_LEN = config['SOURCE_LEN']\n",
    "TARGET_LEN = config['TARGET_LEN']\n",
    "VOCAB_SIZE = config['VOCAB_SIZE']\n",
    "N = config['N']\n",
    "H = config['H']\n",
    "D_MODEL = config['D_MODEL']\n",
    "DROPOUT = config['DROPOUT']\n",
    "D_FF = D_MODEL * 4\n",
    "D_KV = D_MODEL // H # This is default in PyTorch MHA and cannot be set explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2667d499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = spm.SentencePieceProcessor('char_tokenizer/char_tokenizer.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d13c1130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Model\n",
    "model = MODEL(N = N, SOURCE_LEN = SOURCE_LEN, TARGET_LEN = TARGET_LEN,\n",
    "              VOCAB_SIZE = VOCAB_SIZE, D_MODEL = D_MODEL, D_FF = D_FF, D_KV = D_KV,\n",
    "              H = H, DROPOUT = DROPOUT)\n",
    "# Dummy forward pass to initiate the weights before loading\n",
    "X_enc = np.random.randint(low = 0, high = VOCAB_SIZE, size = (1, SOURCE_LEN))\n",
    "X_dec = np.random.randint(low = 0, high = VOCAB_SIZE, size = (1, TARGET_LEN))\n",
    "_ = model((X_enc, X_dec))\n",
    "\n",
    "model.load_weights(f'checkpoints_large/checkpoint_epoch_0010.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10c0fbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(text):\n",
    "    tokenized = tokenizer.encode_as_ids(text)\n",
    "    encoder_input = tf.keras.preprocessing.sequence.pad_sequences([tokenized], maxlen = SOURCE_LEN, \n",
    "                                                            padding = 'post', truncating = 'post')\n",
    "    decoder_input = [2]\n",
    "    decoder_input = tf.keras.preprocessing.sequence.pad_sequences([decoder_input], maxlen = TARGET_LEN, \n",
    "                                                            padding = 'post', truncating = 'post')\n",
    "    for idx in range(TARGET_LEN - 1):\n",
    "        logits = model((encoder_input, decoder_input))\n",
    "        predicted_token = tf.argmax(logits, axis = -1).numpy()[0][idx]\n",
    "        if predicted_token == 3:\n",
    "            break\n",
    "        decoder_input[0, idx+1] = predicted_token\n",
    "        \n",
    "    arr = decoder_input[0]\n",
    "    return tokenizer.decode_ids(arr[arr>3].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00a6a174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can write your test cases here:\n",
    "test_cases = [\"merhaba benim adım melikşah türker 30 yaşındayım istanbulda yaşıyorumm\",\n",
    "              \"boğaziçi üniversitesinde doktora yapıyrum\",\n",
    "              \"kumru hanim adında 4 yasinda bi kedi kizim var\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0af5e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merhaba benim adım Melikşah Türker 30 yaşındayım, İstanbul'da yaşıyorum.\n",
      "Boğaziçi Üniversitesi'nde doktora yapıyrum.\n",
      "Kumru Hanim adında 4 yasinda bi kedi kizim var.\n"
     ]
    }
   ],
   "source": [
    "for case in test_cases:\n",
    "    print(infer(case))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70434387",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
